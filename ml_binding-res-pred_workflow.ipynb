{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% imports\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt_pr\n",
    "import matplotlib.pyplot as plt_roc\n",
    "import matplotlib.pyplot as plt_confm\n",
    "import matplotlib.pyplot as plt_confm_n\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks, CondensedNearestNeighbour\n",
    "\n",
    "from split import build_splits, calculate_se\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef, make_scorer, confusion_matrix, accuracy_score, precision_score, \\\n",
    "    recall_score, balanced_accuracy_score, roc_auc_score, precision_recall_curve, plot_precision_recall_curve, average_precision_score, \\\n",
    "    roc_curve, plot_roc_curve, f1_score, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%% get splits\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "34473\n",
      "34473\n",
      "Split 0: \n",
      "25260\n",
      "25260\n",
      "Split 1: \n",
      "27142\n",
      "27142\n",
      "Split 2: \n",
      "26950\n",
      "26950\n",
      "Split 3: \n",
      "26561\n",
      "26561\n",
      "Split 4: \n",
      "26382\n",
      "26382\n"
     ]
    }
   ],
   "source": [
    "embeddings_file = '../data/MSA1_binding_sites.h5'\n",
    "msa_type = embeddings_file.split(\"/\")[2].split(\"_\")[0]\n",
    "json_file = '../dataset-analysis/split_21-06-12_18-05_0-9713.json'\n",
    "\n",
    "X_test, y_test, all_X_train, all_y_train = build_splits(\n",
    "    json_file,\n",
    "    # '../data/baseline_embeddings_binding_sites.h5',\n",
    "    embeddings_file,\n",
    "    '../data/binding_residues.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%% generate split indices\n"
    }
   },
   "outputs": [],
   "source": [
    "lengths = list(map(len, all_X_train))\n",
    "total_length = sum(lengths)\n",
    "folds = np.zeros(shape=total_length)\n",
    "i = 0\n",
    "for x, length in enumerate(lengths):\n",
    "    folds[i:(i+length)] = x\n",
    "    i += length\n",
    "\n",
    "old_train_set_size = len(folds)\n",
    "\n",
    "# transform into 1 big matrix (split into list of vectors??)\n",
    "big_mac = np.concatenate(all_X_train)\n",
    "big_mac_y = np.concatenate(all_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% training rotation (if evaluation of multiple metrics for training is required for comparing models)\n"
    }
   },
   "outputs": [],
   "source": [
    "for ts in range (0, len(lengths)):\n",
    "    if ts == 0:\n",
    "        # 1st training split as test\n",
    "        y_test = big_mac_y[0:lengths[0]]\n",
    "        X_test = big_mac[0:lengths[0]]\n",
    "        big_mac = big_mac[lengths[0]:]\n",
    "        big_mac_y = big_mac_y[lengths[0]:]\n",
    "        folds = folds[lengths[0]:]\n",
    "        for i in range(0, len(folds)):\n",
    "            folds[i] = folds[i] - 1\n",
    "    elif ts == 1:\n",
    "        # 2nd training split as test\n",
    "        y_test = big_mac_y[lengths[0]:(lengths[0]+lengths[1])]\n",
    "        X_test = big_mac[lengths[0]:(lengths[0]+lengths[1])]\n",
    "        big_mac = np.concatenate((big_mac[0:lengths[0]], big_mac[(lengths[0]+lengths[1]):]))\n",
    "        big_mac_y = np.concatenate((big_mac_y[0:lengths[0]], big_mac_y[(lengths[0]+lengths[1]):]))\n",
    "\n",
    "        folds = np.concatenate((folds[0:lengths[0]], folds[(lengths[0]+lengths[1]):]))\n",
    "    elif ts == 2:\n",
    "        # 3rd training split as test\n",
    "        y_test = big_mac_y[(lengths[0]+lengths[1]):(lengths[0]+lengths[1]+lengths[2])]\n",
    "        X_test = big_mac[(lengths[0]+lengths[1]):(lengths[0]+lengths[1]+lengths[2])]\n",
    "        big_mac = np.concatenate((big_mac[0:(lengths[0]+lengths[1])], big_mac[(lengths[0]+lengths[1]+lengths[2]):]))\n",
    "        big_mac_y = np.concatenate((big_mac_y[0:(lengths[0]+lengths[1])], big_mac_y[(lengths[0]+lengths[1]+lengths[2]):]))\n",
    "\n",
    "        folds = np.concatenate((folds[0:(lengths[0]+lengths[1])], folds[(lengths[0]+lengths[1]+lengths[2]):]))\n",
    "    elif ts == 3:\n",
    "        # 4th training split as test\n",
    "        y_test = big_mac_y[(lengths[0]+lengths[1]+lengths[2]):(lengths[0]+lengths[1]+lengths[2]+lengths[3])]\n",
    "        X_test = big_mac[(lengths[0]+lengths[1]+lengths[2]):(lengths[0]+lengths[1]+lengths[2]+lengths[3])]\n",
    "        big_mac = np.concatenate((big_mac[0:(lengths[0]+lengths[1]+lengths[2])],\n",
    "                                  big_mac[(lengths[0]+lengths[1]+lengths[2]+lengths[3]):]))\n",
    "        big_mac_y = np.concatenate((big_mac_y[0:(lengths[0]+lengths[1]+lengths[2])],\n",
    "                                    big_mac_y[(lengths[0]+lengths[1]+lengths[2]+lengths[3]):]))\n",
    "\n",
    "        folds = np.concatenate((folds[0:(lengths[0]+lengths[1]+lengths[2])],\n",
    "                                folds[(lengths[0]+lengths[1]+lengths[2]+lengths[3]):]))\n",
    "    elif ts == 4:\n",
    "        # 5th training split as test\n",
    "        y_test = big_mac_y[(lengths[0]+lengths[1]+lengths[2]+lengths[3]):]\n",
    "        X_test = big_mac[(lengths[0]+lengths[1]+lengths[2]+lengths[3]):]\n",
    "        big_mac = big_mac[0:(lengths[0]+lengths[1]+lengths[2]+lengths[3])]\n",
    "        big_mac_y = big_mac_y[0:(lengths[0]+lengths[1]+lengths[2]+lengths[3])]\n",
    "\n",
    "        folds = folds[0:(lengths[0]+lengths[1]+lengths[2]+lengths[3])]\n",
    "    if ts > 0:\n",
    "        old_index = folds[0]\n",
    "        x = 0\n",
    "        for i in range(0, len(folds)):\n",
    "            if old_index != folds[i]:\n",
    "                # new fold\n",
    "                x += 1\n",
    "                old_index = folds[i]\n",
    "            folds[i] = x\n",
    "    print(folds)\n",
    "    new_train_set_size = len(folds)\n",
    "    print(str(len(folds)))\n",
    "    assert (old_train_set_size - new_train_set_size) == len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% create predefined split folds\n"
    }
   },
   "outputs": [],
   "source": [
    "ps = PredefinedSplit(folds)\n",
    "print(\"n splits: \" + str(ps.get_n_splits()))\n",
    "\n",
    "for train_index, val_index in ps.split():\n",
    "     print(\"TRAIN:\", train_index, \"TEST:\", val_index)\n",
    "     X_train, X_val = big_mac[train_index], big_mac[val_index]\n",
    "     y_train, y_val = big_mac_y[train_index], big_mac_y[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% ML\n"
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(activation='relu', solver='adam', early_stopping=True, learning_rate='invscaling',\n",
    "                    hidden_layer_sizes=(80,),\n",
    "                    alpha=0.001,\n",
    "                    #learning_rate_init=0.00075,\n",
    "                    max_iter=50,\n",
    "                    random_state=42,\n",
    "                   )\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1,\n",
    "                             # n_estimators=70,\n",
    "                             # class_weight='balanced_subsample'\n",
    "                            )\n",
    "\n",
    "linsvc = LinearSVC(max_iter=1000, random_state=0)\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4) # only for testing\n",
    "\n",
    "# param grid: add (lowercase) typename__ in front of every hyperparameter\n",
    "params = {# MLP\n",
    "          #'mlpclassifier__hidden_layer_sizes': [(80,),],  # mlpclassifier__\n",
    "          #'mlpclassifier__max_iter': [50,],\n",
    "          #'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "          #'mlpclassifier__learning_rate_init': [0.001, 0.0005],\n",
    "          #'mlpclassifier__alpha': [0.0001,0.001],\n",
    "          # RFC\n",
    "          #'randomforestclassifier__max_depth': [20, 30],  # randomforestclassifier__\n",
    "          #'randomforestclassifier__n_estimators': [70, 90],\n",
    "          #'class_weight': [\"balanced\", \"balanced_subsample\", \"None\"],\n",
    "          # SVM\n",
    "          #'C': [1.0, 4.0 ],  # linearsvc__\n",
    "          # RUS\n",
    "          'randomundersampler__sampling_strategy': [0.2],\n",
    "          }\n",
    "\n",
    "# imba_pipeline = make_pipeline(SMOTE(random_state=42, sampling_strategy=0.1, n_jobs=-1), mlp)\n",
    "imba_pipeline = make_pipeline(RandomUnderSampler(random_state=42), mlp)\n",
    "# to try: sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "gs = GridSearchCV(estimator=imba_pipeline, cv=ps, param_grid=params,\n",
    "                  scoring=mcc_scorer, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% train\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"*TRAINING*\\t\" + str(datetime.now()))\n",
    "\n",
    "# gs.fit(big_mac, big_mac_y)\n",
    "gs.fit(all_X_train[2], all_y_train[2])\n",
    "best_score = round(gs.best_score_, 5)\n",
    "\n",
    "# print results\n",
    "print(\"    Classifier:  \" + str(gs.estimator))\n",
    "print(\"    Best score:  %f\" % best_score)\n",
    "print(\"    Best params: %s\" % str(gs.best_params_))\n",
    "cv_results = pd.DataFrame(gs.cv_results_)\n",
    "print(f\"   \\nAll training results:\\n{cv_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% save trained GridSearch\n"
    }
   },
   "outputs": [],
   "source": [
    "# pickle_file_name = \"gs_\" + msa_type + \"_\" + str(best_score).replace(\".\", \"-\") + \".pickle\"\n",
    "# with open((\"../data/pickles/\" + pickle_file_name), \"wb\") as pickle_file:\n",
    "#    pickle.dump(gs, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% predict\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"*TESTING*\\t\" + str(datetime.now()))\n",
    "\n",
    "best_classifier = gs.best_estimator_\n",
    "print(best_classifier)\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "y_proba = gs.predict_proba(X_test).transpose()[1].transpose()  # assumes 2nd column corresponds to class 1\n",
    "pred_score = best_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% evaluate\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"*RESULTS*\\t\" + str(datetime.now()))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "perf_metrics = [0]*7\n",
    "perf_metrics[0] = matthews_corrcoef(y_test, y_pred)\n",
    "perf_metrics[1] = accuracy_score(y_test, y_pred)\n",
    "perf_metrics[2] = precision_score(y_test, y_pred)\n",
    "perf_metrics[3] = recall_score(y_test, y_pred)\n",
    "perf_metrics[4] = roc_auc_score(y_true=y_test, y_score=y_proba)\n",
    "perf_metrics[5] = balanced_accuracy_score(y_test, y_pred)\n",
    "perf_metrics[6] = f1_score(y_test, y_pred)\n",
    "# print(\"    Pred:        \" + str(pred_score))  # should be MCC but is accuracy?\n",
    "print(\"    MCC:         \" + str(perf_metrics[0]))\n",
    "print(\"    Accuracy:    \" + str(perf_metrics[1]))\n",
    "print(\"    Precision:   \" + str(perf_metrics[2]))\n",
    "print(\"    Recall:      \" + str(perf_metrics[3]))\n",
    "print(\"    AUC-ROC:     \" + str(perf_metrics[4]))\n",
    "print(\"    BalancedAcc: \" + str(perf_metrics[5]))\n",
    "print(\"    F1 Score:    \" + str(perf_metrics[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% plot evaluation graphics\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Precision-Recall curve:\")\n",
    "prec_score, rec_score, thresholds_pr = precision_recall_curve(y_true=y_test, probas_pred=y_proba)\n",
    "av_prec_score = average_precision_score(y_true=y_test, y_score=y_proba)\n",
    "disp = plot_precision_recall_curve(gs, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(av_prec_score))\n",
    "\n",
    "print(\"ROC curve:\")\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_true=y_test, y_score=y_proba)\n",
    "disp2 = plot_roc_curve(gs, X_test, y_test)\n",
    "disp2.ax_.set_title('2-class ROC curve: '\n",
    "                   'AUC={0:0.2f}'.format(perf_metrics[4]))\n",
    "plt_roc.show()\n",
    "\n",
    "print(\"Confusion matrices:\")\n",
    "class_names = ['non_binding', 'binding']\n",
    "plot_confusion_matrix(gs, X_test, y_test, display_labels=class_names)\n",
    "plt_confm.show()\n",
    "plot_confusion_matrix(gs, X_test, y_test, display_labels=class_names,\n",
    "                     normalize='true')\n",
    "plt_confm_n.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% calculate CI/SE (without bootstrapping)\n"
    }
   },
   "outputs": [],
   "source": [
    "standard_errors = calculate_se(json_file, embeddings_file, y_pred, y_test)\n",
    "\n",
    "\n",
    "# remove ROC AUC (no SE) from metrics\n",
    "del perf_metrics[4]\n",
    "\n",
    "print(\"Confidence intervals and standard errors:\")\n",
    "conf_intervals = [0]*len(standard_errors)\n",
    "for i in range(0, len(perf_metrics)):\n",
    "    # CI: use student's t-distriution for 195-1 = 194 = ~200 DOF and conf level = 0.95 -> 1.972\n",
    "    # formula: mean +- t*SE\n",
    "    conf_intervals[i] = 1.972 * standard_errors[i]\n",
    "    print(\"\\t\" + str(round(perf_metrics[i], 5)) + \"\\t +/- \" + str(conf_intervals[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% baseline\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Comparison to baseline:\")\n",
    "n = len(y_test)\n",
    "probabilities = np.array([0.9185, 0.0815])\n",
    "y_pred_random = np.zeros(shape=n)\n",
    "for i in range(0, n):\n",
    "    y_pred_random[i] = np.random.choice(a=[0, 1], p=probabilities)\n",
    "\n",
    "y_pred_majclass = np.zeros(shape=len(y_test))\n",
    "print(\"    Random:\")\n",
    "print(\"\\tMCC:         \" + str(matthews_corrcoef(y_test, y_pred_random)))\n",
    "print(\"\\tAccuracy:    \" + str(accuracy_score(y_test, y_pred_random)))\n",
    "print(\"\\tPrecision:   \" + str(precision_score(y_test, y_pred_random)))\n",
    "print(\"\\tRecall:      \" + str(recall_score(y_test, y_pred_random)))\n",
    "print(\"\\tBalancedAcc: \" + str(balanced_accuracy_score(y_test, y_pred_random)))\n",
    "print(\"\\tF1 score:    \" + str(f1_score(y_test, y_pred_random)))\n",
    "print(\"    Majority class [0]:\")\n",
    "print(\"\\tMCC:         \" + str(matthews_corrcoef(y_test, y_pred_majclass)))\n",
    "print(\"\\tAccuracy:    \" + str(accuracy_score(y_test, y_pred_majclass)))\n",
    "print(\"\\tPrecision:   \" + str(precision_score(y_test, y_pred_majclass)))\n",
    "print(\"\\tRecall:      \" + str(recall_score(y_test, y_pred_majclass)))\n",
    "print(\"\\tBalancedAcc: \" + str(balanced_accuracy_score(y_test, y_pred_majclass)))\n",
    "print(\"\\tF1 score:    \" + str(f1_score(y_test, y_pred_majclass)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
